{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clustering.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWTvVkGZmNetkxRQCasxil",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f6efa37f370c426aa3b9d8496b62d323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e1825c1b0f444ca9df201663b6c12d7",
              "IPY_MODEL_7c56f5d2cfcc4c308aa8c6e7cecda072",
              "IPY_MODEL_39b4be3bd5c641ad877ffe04c9905df4"
            ],
            "layout": "IPY_MODEL_e588395135ab458c950662d8411f02a9"
          }
        },
        "5e1825c1b0f444ca9df201663b6c12d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38a5fda0a3c4478ca4369edad3b220f1",
            "placeholder": "​",
            "style": "IPY_MODEL_0855cb124d1e4eafaf51d1eaf464a0c2",
            "value": "Epochs completed: 100%| "
          }
        },
        "7c56f5d2cfcc4c308aa8c6e7cecda072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ac4799a01204b298cfcb8736880de0c",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56d09d1827bd41d99a19464ac05419f5",
            "value": 100
          }
        },
        "39b4be3bd5c641ad877ffe04c9905df4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eea131294b15435a9d349ce4054b247c",
            "placeholder": "​",
            "style": "IPY_MODEL_424a94bf208a41b88c8b8e6b6db0c53d",
            "value": " 100/100 [00:02]"
          }
        },
        "e588395135ab458c950662d8411f02a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38a5fda0a3c4478ca4369edad3b220f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0855cb124d1e4eafaf51d1eaf464a0c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ac4799a01204b298cfcb8736880de0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56d09d1827bd41d99a19464ac05419f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eea131294b15435a9d349ce4054b247c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "424a94bf208a41b88c8b8e6b6db0c53d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prith189/GLG_DL/blob/main/Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZpQ71-0cCV5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline suggested by BERTopic:\n",
        "\n",
        "- Generate embeddings using the Sentence Transformer model (Each block of text is converted to a 384 dimensional vector)\n",
        "\n",
        "- Reduce the dimensionality using UMAP for 384 dimensions to 5 dimensions\n",
        "\n",
        "- Cluster the 5 dimensional vectors using HDBSCAN\n",
        "\n",
        "- For each cluster, run TF-IDF to generate a representation of the topic\n",
        "\n",
        "\n",
        "Changes made to use the News dataset\n",
        "\n",
        "- For clustering, HDBSCAN classifies most of the vectors in the embedded space as noise\n",
        "\n",
        "- Kmeans clusters all data points into clusters, therefore KMeans was used\n",
        "\n",
        "- In the below notebook, UMAP was used for dimensionality reduction and Kmeans was used for clustering"
      ],
      "metadata": {
        "id": "JbOIkY2eGZwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RUN_SENTENCE_TRANSFORMER = False #Set this to True if we need to generate embeddings from scratch. Requires GPU else very slow.\n",
        "RUN_UMAP = False #Set this to True if we need to reduce the dimensionality of the embeddings using UMAP (Requires >100GB of RAM to run for all data points)\n",
        "RUN_KMEANS = False #Set this to True if we need to run KMeans on the reduce dimension vectors\n",
        "EXTRACT_TOPICS = False #Set this to True to extract a description of each of the topics\n",
        "LABEL_TOPICS = False\n",
        "TEST_NEW_TEXT = True #To test out new topics"
      ],
      "metadata": {
        "id": "T6s_D_VcDGyC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ndoYr_ndCNy",
        "outputId": "bef2ee6f-310d-4448-8c33-508dd8180e57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 34.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.64.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=7e06304fc8c3ae2bfe9269b6865dd83508fa2d9cd8f4601e3639a5bda758aeef\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=d5875eddb23a4de9769f4ca8de0cae3b30e5a31727a6b63e2ccbf6c3672eda1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.6 umap-learn-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece] sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMngECgWd8Gu",
        "outputId": "a45b555f-439e-4704-df19-95ad9e82ab2a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 16.2 MB/s \n",
            "\u001b[?25hCollecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 77.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 87.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 91.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 65.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->sentence-transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=9840a7b84468516266115c0d1195a33c7211a735911c5d5c0d5d05ea18bba2b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=ede7adf295d011ada7a5996f0c5c0e90c032a4d6b3df4c5d22b8ceb46f8e60d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.53 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_drive = True"
      ],
      "metadata": {
        "id": "aVObnKdermt0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(use_drive):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_PATH = '/content/drive/My Drive/fourthbrain/'\n",
        "else:\n",
        "    BASE_PATH = '/content/'"
      ],
      "metadata": {
        "id": "1jaZ5aaVqCWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91432e6d-cb70-4fbb-a4cc-887506a3a2bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pjRzTHFltMxb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EMBEDDINGS FILE\n",
        "#!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-3gKYoipfdPkeQHnHa0M2vD7QEug5wNS' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-3gKYoipfdPkeQHnHa0M2vD7QEug5wNS\" -O all-the-news-embeddings-title.npy && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "id": "q5Xh6qePlYj3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#INDEX FILE for NEWS DATASET (so that the embeddings file matches the entries from the news.csv file)\n",
        "#!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-5IsScXPtUY5jXVe_83RuQ0uI7eQqDMI' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-5IsScXPtUY5jXVe_83RuQ0uI7eQqDMI\" -O all-the-news-embeddings-title-index.npy && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "id": "XiYN4sZSl3Ny"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Cr0YuS85hynqfi_4_Kr99h4HTTUpsZ-u' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Cr0YuS85hynqfi_4_Kr99h4HTTUpsZ-u\" -O all-the-news-2-1.csv && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "id": "TUy-wUptqppS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "class FeatureExtraction:\n",
        "    def __init__(self):\n",
        "        #Load the pretrained model\n",
        "        self.fe = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=0)\n",
        "\n",
        "    def run_fe_batch(self, list_of_input_text):\n",
        "        list_of_fe_vec = self.fe.encode(list_of_input_text, show_progress_bar=False)\n",
        "        return list_of_fe_vec\n",
        "\n",
        "\n",
        "class NewsDataset:\n",
        "    def __init__(self):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.preprocess()\n",
        "        self.ner = None\n",
        "    \n",
        "    def preprocess(self):\n",
        "        self.df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1','date','year','month','day','article','publication'], inplace=True)\n",
        "        print('Shape of dataframe before dropping nan:{}'.format(self.df.shape))\n",
        "        self.df = self.df.dropna(subset=['title'])\n",
        "        print('Shape of dataframe after dropping nan:{}'.format(self.df.shape))"
      ],
      "metadata": {
        "id": "HJew44yNfXPk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_file = os.path.join(BASE_PATH, 'all-the-news-embeddings-title.npy')\n",
        "idx_file = os.path.join(BASE_PATH, 'all-the-news-embeddings-title-index.npy')\n",
        "if(RUN_SENTENCE_TRANSFORMER):    \n",
        "    feature_extractor = FeatureExtraction()\n",
        "    news = NewsDataset()\n",
        "    df_text = news.df['title'].to_list()\n",
        "    features = feature_extractor.run_fe_batch(df_text)\n",
        "    df_idx = news.df.index\n",
        "    np.save(features_file, features)\n",
        "    np.save(idx_file, df_idx)\n",
        "else:\n",
        "    features = np.load(features_file)\n",
        "    df_idx = np.load(idx_file)"
      ],
      "metadata": {
        "id": "dctgR57meE-2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim_red_embeddings_file = os.path.join(BASE_PATH, 'all-the-news-embeddings-title-umap.npy')\n",
        "umap_model_file = os.path.join(BASE_PATH, 'umap-model.sav')\n",
        "if(RUN_UMAP):\n",
        "    from umap import UMAP\n",
        "    umap_model = UMAP(n_neighbors=15,n_components=5,min_dist=0.0,metric='cosine',low_memory=True, verbose=True)\n",
        "    umap_model.fit(features)\n",
        "    dim_red_embeddings = umap_model.transform(features)\n",
        "    np.save(dim_red_embeddings_file, dim_red_embeddings)\n",
        "    #f = open(umap_model_file, 'wb')\n",
        "    #pickle.dump(umap_model, f)\n",
        "    #f.close()\n",
        "    joblib.dump(umap_model, umap_model_file, protocol=4)\n",
        "else:\n",
        "    dim_red_embeddings = np.load(dim_red_embeddings_file)\n",
        "    #f = open(umap_model_file, 'rb')\n",
        "    #umap_model_2 = pickle.load(f)\n",
        "    #f.close()\n",
        "    umap_model = joblib.load(umap_model_file)"
      ],
      "metadata": {
        "id": "N_ISZu83aXgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1dbc6d7-474d-4bb1-ee90-720e1d94262e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 11 18:14:32 2022 Building and compiling search function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pydrive.auth import GoogleAuth\n",
        "# from pydrive.drive import GoogleDrive\n",
        "# from google.colab import auth\n",
        "# from oauth2client.client import GoogleCredentials"
      ],
      "metadata": {
        "id": "WtIyOjXSbeFA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# auth.authenticate_user()\n",
        "# gauth = GoogleAuth()\n",
        "# gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "uIaKQMbCb9NT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uploaded = drive.CreateFile({'title': 'all-the-news-embeddings-title-umap.npy'})\n",
        "# uploaded.SetContentFile('/content/embeddings-title-umap.npy')\n",
        "# uploaded.Upload()"
      ],
      "metadata": {
        "id": "lC2YBCj5cOIl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans_model_file = os.path.join(BASE_PATH, 'kmeans_model.p')\n",
        "labels_file = os.path.join(BASE_PATH, 'umap-kmeans-labels.npy')\n",
        "import pickle as p\n",
        "if(RUN_KMEANS):\n",
        "    from sklearn.cluster import MiniBatchKMeans\n",
        "    kmn = MiniBatchKMeans(n_clusters=25, verbose=1)\n",
        "    labels = kmn.fit_predict(dim_red_embeddings)\n",
        "    print('Number of datapoints in each cluster---')\n",
        "    print(np.unique(labels, return_counts=True))\n",
        "    f = open(kmeans_model_file, 'wb')\n",
        "    pickle.dump(kmn, f)\n",
        "    f.close()\n",
        "    np.save(labels_file, labels)\n",
        "else:\n",
        "    f = open(kmeans_model_file, 'rb')\n",
        "    kmn = pickle.load(f)\n",
        "    f.close()\n",
        "    labels = np.load(labels_file)"
      ],
      "metadata": {
        "id": "7UzamtU4qIS9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The following functions were copied from the BERTopic module to extract topic descriptions from a set of clusters\n",
        "\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.utils import check_array\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "\n",
        "class ClassTFIDF(TfidfTransformer):\n",
        "    \"\"\"\n",
        "    A Class-based TF-IDF procedure using scikit-learns TfidfTransformer as a base.\n",
        "    ![](../img/ctfidf.png)\n",
        "    C-TF-IDF can best be explained as a TF-IDF formula adopted for multiple classes\n",
        "    by joining all documents per class. Thus, each class is converted to a single document\n",
        "    instead of set of documents. Then, the frequency of words **t** are extracted for\n",
        "    each class **i** and divided by the total number of words **w**.\n",
        "    Next, the total, unjoined, number of documents across all classes **m** is divided by the total\n",
        "    sum of word **i** across all classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(ClassTFIDF, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def fit(self, X, multiplier):\n",
        "        \"\"\"Learn the idf vector (global term weights).\n",
        "        Arguments:\n",
        "            X: A matrix of term/token counts.\n",
        "            multiplier: A multiplier for increasing/decreasing certain IDF scores\n",
        "        \"\"\"\n",
        "        X = check_array(X, accept_sparse=('csr', 'csc'))\n",
        "        if not sp.issparse(X):\n",
        "            X = sp.csr_matrix(X)\n",
        "        dtype = np.float64\n",
        "\n",
        "        if self.use_idf:\n",
        "            _, n_features = X.shape\n",
        "\n",
        "            # Calculate the frequency of words across all classes\n",
        "            df = np.squeeze(np.asarray(X.sum(axis=0)))\n",
        "\n",
        "            # Calculate the average number of samples as regularization\n",
        "            avg_nr_samples = int(X.sum(axis=1).mean())\n",
        "\n",
        "            # Divide the average number of samples by the word frequency\n",
        "            # +1 is added to force values to be positive\n",
        "            idf = np.log((avg_nr_samples / df)+1)\n",
        "\n",
        "            # Multiplier to increase/decrease certain idf scores\n",
        "            if multiplier is not None:\n",
        "                idf = idf * multiplier\n",
        "\n",
        "            self._idf_diag = sp.diags(idf, offsets=0,\n",
        "                                      shape=(n_features, n_features),\n",
        "                                      format='csr',\n",
        "                                      dtype=dtype)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform a count-based matrix to c-TF-IDF\n",
        "        Arguments:\n",
        "            X (sparse matrix): A matrix of term/token counts.\n",
        "        Returns:\n",
        "            X (sparse matrix): A c-TF-IDF matrix\n",
        "        \"\"\"\n",
        "        if self.use_idf:\n",
        "            X = normalize(X, axis=1, norm='l1', copy=False)\n",
        "            X = X * self._idf_diag\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "def c_tf_idf(documents_per_topic):\n",
        "    \"\"\" Calculate a class-based TF-IDF where m is the number of total documents.\n",
        "    Arguments:\n",
        "        documents_per_topic: The joined documents per topic such that each topic has a single\n",
        "                              string made out of multiple documents\n",
        "        m: The total number of documents (unjoined)\n",
        "        fit: Whether to fit a new vectorizer or use the fitted self.vectorizer_model\n",
        "    Returns:\n",
        "        tf_idf: The resulting matrix giving a value (importance score) for each word per topic\n",
        "        words: The names of the words to which values were given\n",
        "    \"\"\"\n",
        "    documents = preprocess_text(documents_per_topic['title'].values)\n",
        "\n",
        "    vectorizer_model = CountVectorizer(ngram_range=(1,2), stop_words='english')\n",
        "\n",
        "    vectorizer_model.fit(documents)\n",
        "\n",
        "    words = vectorizer_model.get_feature_names()\n",
        "    X = vectorizer_model.transform(documents)\n",
        "\n",
        "    transformer = ClassTFIDF().fit(X, multiplier=None)\n",
        "\n",
        "    c_tf_idf = transformer.transform(X)\n",
        "\n",
        "    topic_sim_matrix = cosine_similarity(c_tf_idf)\n",
        "\n",
        "    return c_tf_idf, words\n",
        "\n",
        "def preprocess_text(documents):\n",
        "    \"\"\" Basic preprocessing of text\n",
        "    Steps:\n",
        "        * Lower text\n",
        "        * Replace \\n and \\t with whitespace\n",
        "        * Only keep alpha-numerical characters\n",
        "    \"\"\"\n",
        "    cleaned_documents = [doc.lower() for doc in documents]\n",
        "    cleaned_documents = [doc.replace(\"\\n\", \" \") for doc in cleaned_documents]\n",
        "    cleaned_documents = [doc.replace(\"\\t\", \" \") for doc in cleaned_documents]\n",
        "    cleaned_documents = [re.sub(r'[^A-Za-z0-9 ]+', '', doc) for doc in cleaned_documents]\n",
        "    cleaned_documents = [doc if doc != \"\" else \"emptydoc\" for doc in cleaned_documents]\n",
        "    return cleaned_documents\n",
        "\n",
        "def top_n_idx_sparse(matrix, n):\n",
        "    \"\"\" Return indices of top n values in each row of a sparse matrix\n",
        "    Retrieved from:\n",
        "        https://stackoverflow.com/questions/49207275/finding-the-top-n-values-in-a-row-of-a-scipy-sparse-matrix\n",
        "    Arguments:\n",
        "        matrix: The sparse matrix from which to get the top n indices per row\n",
        "        n: The number of highest values to extract from each row\n",
        "    Returns:\n",
        "        indices: The top n indices per row\n",
        "    \"\"\"\n",
        "    indices = []\n",
        "    for le, ri in zip(matrix.indptr[:-1], matrix.indptr[1:]):\n",
        "        n_row_pick = min(n, ri - le)\n",
        "        values = matrix.indices[le + np.argpartition(matrix.data[le:ri], -n_row_pick)[-n_row_pick:]]\n",
        "        values = [values[index] if len(values) >= index + 1 else None for index in range(n)]\n",
        "        indices.append(values)\n",
        "    return np.array(indices)\n",
        "\n",
        "def top_n_values_sparse(matrix, indices):\n",
        "    \"\"\" Return the top n values for each row in a sparse matrix\n",
        "    Arguments:\n",
        "        matrix: The sparse matrix from which to get the top n indices per row\n",
        "        indices: The top n indices per row\n",
        "    Returns:\n",
        "        top_values: The top n scores per row\n",
        "    \"\"\"\n",
        "    top_values = []\n",
        "    for row, values in enumerate(indices):\n",
        "        scores = np.array([matrix[row, value] if value is not None else 0 for value in values])\n",
        "        top_values.append(scores)\n",
        "    return np.array(top_values)\n",
        "\n",
        "def extract_words_per_topic(words,c_tf_idf,labels):\n",
        "        \"\"\" Based on tf_idf scores per topic, extract the top n words per topic\n",
        "        If the top words per topic need to be extracted, then only the `words` parameter\n",
        "        needs to be passed. If the top words per topic in a specific timestamp, then it\n",
        "        is important to pass the timestamp-based c-TF-IDF matrix and its corresponding\n",
        "        labels.\n",
        "        Arguments:\n",
        "            words: List of all words (sorted according to tf_idf matrix position)\n",
        "            c_tf_idf: A c-TF-IDF matrix from which to calculate the top words\n",
        "            labels: A list of topic labels\n",
        "        Returns:\n",
        "            topics: The top words per topic\n",
        "        \"\"\"\n",
        "\n",
        "        # Get the top 30 indices and values per row in a sparse c-TF-IDF matrix\n",
        "        indices = top_n_idx_sparse(c_tf_idf, 30)\n",
        "        scores = top_n_values_sparse(c_tf_idf, indices)\n",
        "        sorted_indices = np.argsort(scores, 1)\n",
        "        indices = np.take_along_axis(indices, sorted_indices, axis=1)\n",
        "        scores = np.take_along_axis(scores, sorted_indices, axis=1)\n",
        "\n",
        "        # Get top 30 words per topic based on c-TF-IDF score\n",
        "        topics = {label: [(words[word_index], score)\n",
        "                          if word_index is not None and score > 0\n",
        "                          else (\"\", 0.00001)\n",
        "                          for word_index, score in zip(indices[index][::-1], scores[index][::-1])\n",
        "                          ]\n",
        "                  for index, label in enumerate(labels)}\n",
        "\n",
        "        # Extract word embeddings for the top 30 words per topic and compare it\n",
        "        # with the topic embedding to keep only the words most similar to the topic embedding\n",
        "        # if self.diversity is not None:\n",
        "        #     if self.embedding_model is not None:\n",
        "\n",
        "        #         for topic, topic_words in topics.items():\n",
        "        #             words = [word[0] for word in topic_words]\n",
        "        #             word_embeddings = self._extract_embeddings(words,\n",
        "        #                                                        method=\"word\",\n",
        "        #                                                        verbose=False)\n",
        "        #             topic_embedding = self._extract_embeddings(\" \".join(words),\n",
        "        #                                                        method=\"word\",\n",
        "        #                                                        verbose=False).reshape(1, -1)\n",
        "        #             topic_words = mmr(topic_embedding, word_embeddings, words,\n",
        "        #                               top_n=self.top_n_words, diversity=self.diversity)\n",
        "        #             topics[topic] = [(word, value) for word, value in topics[topic] if word in topic_words]\n",
        "        # topics = {label: values[:self.top_n_words] for label, values in topics.items()}\n",
        "\n",
        "        return topics"
      ],
      "metadata": {
        "id": "vqHqUHKlN1Ah"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics_file = os.path.join(BASE_PATH, 'umap-kmeans-topics.p')\n",
        "if(EXTRACT_TOPICS):\n",
        "    csv_file = os.path.join(BASE_PATH, 'all-the-news-2-1.csv')\n",
        "    df = pd.read_csv(csv_file, usecols=['title'])\n",
        "    df = df.iloc[df_idx]\n",
        "    df['Topic'] = labels\n",
        "    df = df[['title', 'Topic']]\n",
        "    n_topics = df['Topic'].unique().shape[0]\n",
        "    documents_per_topic = df.groupby(['Topic'], as_index=False).agg({'title': ' '.join})\n",
        "    sizes = df.groupby(['Topic']).count().sort_values(\"title\", ascending=False).reset_index()\n",
        "    topic_sizes = dict(zip(sizes['Topic'], sizes['title']))\n",
        "    labels = sorted(list(topic_sizes.keys()))\n",
        "    documents_per_topic = df.groupby(['Topic'], as_index=False).agg({'title': ' '.join})\n",
        "    c_tf_idf_m, words = c_tf_idf(documents_per_topic)\n",
        "    topics = extract_words_per_topic(words, c_tf_idf_m, labels)\n",
        "    f = open(topics_file, 'wb')\n",
        "    pickle.dump(topics, f)\n",
        "    f.close()\n",
        "else:\n",
        "    f = open(topics_file, 'rb')\n",
        "    topics = pickle.load(f)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "AKCrfCVCv6_m"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic in topics:\n",
        "    print('Topic {} Keywords {}'.format(topic, '_'.join([i[0] for i in topics[topic][:10]])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP2mtgC7Oz1C",
        "outputId": "bc75ad2e-b46f-496c-af45-0a671886b3a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0 Keywords uber_tesla_car_cars_selfdriving_electric_elon_musk_techcrunch_elon musk\n",
            "Topic 1 Keywords thehill_police_shooting_trump_man_gun_school_immigration_abortion_opinion\n",
            "Topic 2 Keywords new_art_netflix_review_women_star_season_best_tv_opinion\n",
            "Topic 3 Keywords south_africas_sri_africa_south africas_african_myanmar_nigeria_sudan_philippines\n",
            "Topic 4 Keywords preview_win_past_nfl_olympics_nba_cup_world_olympic_game\n",
            "Topic 5 Keywords brexit_uk_eu_dollar_pm_euro_yields_trade_says_ecb\n",
            "Topic 6 Keywords mln_share_profit_net_pct_says_loss_announces_reports_fy\n",
            "Topic 7 Keywords coronavirus_virus_outbreak_cases_stockstsx_covid19_canada stockstsx_canada_china_coronavirus outbreak\n",
            "Topic 8 Keywords review book_book review_book_review_acquisitions_daymergers acquisitions_daymergers_deals daymergers_deals_acquisitions deals\n",
            "Topic 9 Keywords trump_thehill_russia_impeachment_mueller_house_russian_report_trumps_fbi\n",
            "Topic 10 Keywords techcrunch_apple_amazon_google_iphone_new_samsung_app_microsoft_apples\n",
            "Topic 11 Keywords kardashian_new_prince_kim_star_jenner_thrones_game thrones_video_game\n",
            "Topic 12 Keywords korea_north_north korea_puerto_rico_puerto rico_south_korean_south korea_kim\n",
            "Topic 13 Keywords dog_new_space_food_drone_nasa_thehill_dogs_cat_new york\n",
            "Topic 14 Keywords hurricane_california_marijuana_thehill_storm_new_police_crash_french_macron\n",
            "Topic 15 Keywords ceo_fed_stocks_says_market_bank_street_stock_cramer_wall\n",
            "Topic 16 Keywords thehill_trump_trumps_gop_clinton_house_donald_sanders_tax_democrats\n",
            "Topic 17 Keywords climate_boeing_climate change_airlines_737_air_epa_max_change_737 max\n",
            "Topic 18 Keywords brazil_brazils_venezuela_update_argentina_venezuelas_says_emerging_petrobras_stocksfactors\n",
            "Topic 19 Keywords facebook_techcrunch_twitter_thehill_data_tech_opioid_google_privacy_social\n",
            "Topic 20 Keywords oil_saudi_huawei_crude_update_opec_prices_gas_pipeline_says\n",
            "Topic 21 Keywords china_trade_hong_kong_hong kong_chinas_chinese_tariffs_says_fitch\n",
            "Topic 22 Keywords numbers drawn_winning numbers_drawn_game winning_numbers_winning_game_drawn dc_midday game_midday\n",
            "Topic 23 Keywords iran_syria_turkey_says_syrian_turkish_trump_thehill_isis_russia\n",
            "Topic 24 Keywords best_fashion_beauty_new_recipe_starbucks_makeup_sale_food_coffee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_labels_file = os.path.join(BASE_PATH, 'umap-kmeans-topic-labels.p')\n",
        "topic_labels = {}\n",
        "if(LABEL_TOPICS):\n",
        "    topic_labels[0] = 'TECHNOLOGY'\n",
        "    topic_labels[1] = 'POLITICS'\n",
        "    topic_labels[2] = 'TELEVISION'\n",
        "    topic_labels[3] = 'WORLD AFFAIRS'\n",
        "    topic_labels[4] = 'SPORTS'\n",
        "    topic_labels[5] = 'EUROPE'\n",
        "    topic_labels[6] = 'BUSINESS'\n",
        "    topic_labels[7] = 'PUBLIC HEALTH'\n",
        "    topic_labels[8] = 'LITERATURE'\n",
        "    topic_labels[9] = 'POLITICS'\n",
        "    topic_labels[10] = 'TECHNOLOGY'\n",
        "    topic_labels[11] = 'TELEVISION'\n",
        "    topic_labels[12] = \"WORLD AFFAIRS\"\n",
        "    topic_labels[13] = \"LIFESTYLE\"\n",
        "    topic_labels[14] = 'CLIMATE'\n",
        "    topic_labels[15] = 'BUSINESS'\n",
        "    topic_labels[16] = 'POLTICS'\n",
        "    topic_labels[17] = 'CLIMATE'\n",
        "    topic_labels[18] = 'WORLD AFFAIRS'\n",
        "    topic_labels[19] = 'TECHNOLOGY'\n",
        "    topic_labels[20] = 'ENERGY'\n",
        "    topic_labels[21] = 'WORLD AFFAIRS'\n",
        "    topic_labels[22] = 'LOTTO'\n",
        "    topic_labels[23] = 'WORLD AFFAIRS'\n",
        "    topic_labels[24] = 'LIFESTYLE'\n",
        "    f = open(topic_labels_file, 'wb')\n",
        "    pickle.dump(topic_labels, f)\n",
        "    f.close()\n",
        "else:\n",
        "    f = open(topic_labels_file, 'rb')\n",
        "    topic_labels = pickle.load(f)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "CgEEzvx4OyYH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample_text():\n",
        "    st = []\n",
        "    st.append('Secret Service on the defensive over allegations agents were duped by men impersonating feds') #Government\n",
        "    st.append('Microsoft and other tech firms take aim at prolific cybercrime gang') #Technology\n",
        "    st.append('Phoenix Suns favorites to win NBA title, but they still feel disrespected. Are they overlooked?') #Sports\n",
        "    st.append(\"Natural gas spikes to highest level since 2008 as rare nor'easter looms\") #Business\n",
        "    st.append(\"Will rising prices sink Biden’s midterm hopes for Democrats?\") #Politics\n",
        "    st.append(\"Large and dangerous' tornadoes hit Texas and Oklahoma; South faces more severe weather\") #Climate\n",
        "    st.append(\"Here is a list of the best beaches in Hawaii and other tropical islands\") #Travel\n",
        "    return st"
      ],
      "metadata": {
        "id": "Qx3mQkU5lKWW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(TEST_NEW_TEXT):\n",
        "    feature_extractor = FeatureExtraction()\n",
        "    new_text = get_sample_text()\n",
        "    test_embeddings = feature_extractor.run_fe_batch(new_text)\n",
        "    test_dim_red_embeddings = umap_model.transform(test_embeddings)\n",
        "    test_labels = list(kmn.predict(test_dim_red_embeddings))\n",
        "    for label, text in zip(test_labels, new_text):\n",
        "        print('Test text:', text)\n",
        "        print('Predicted Topic:', topic_labels[label])\n",
        "        print('Predicted keywords:', '_'.join([i[0] for i in topics[label][:5]]))\n",
        "        print('*****************************')"
      ],
      "metadata": {
        "id": "nEMpm54WlELx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "f6efa37f370c426aa3b9d8496b62d323",
            "5e1825c1b0f444ca9df201663b6c12d7",
            "7c56f5d2cfcc4c308aa8c6e7cecda072",
            "39b4be3bd5c641ad877ffe04c9905df4",
            "e588395135ab458c950662d8411f02a9",
            "38a5fda0a3c4478ca4369edad3b220f1",
            "0855cb124d1e4eafaf51d1eaf464a0c2",
            "0ac4799a01204b298cfcb8736880de0c",
            "56d09d1827bd41d99a19464ac05419f5",
            "eea131294b15435a9d349ce4054b247c",
            "424a94bf208a41b88c8b8e6b6db0c53d"
          ]
        },
        "outputId": "d4c971d5-4523-455e-96be-9cb1f00e77d8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epochs completed:   0%|            0/100 [00:00]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6efa37f370c426aa3b9d8496b62d323"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test text: Secret Service on the defensive over allegations agents were duped by men impersonating feds\n",
            "Predicted Topic: POLTICS\n",
            "Predicted keywords: thehill_trump_trumps_gop_clinton\n",
            "*****************************\n",
            "Test text: Microsoft and other tech firms take aim at prolific cybercrime gang\n",
            "Predicted Topic: TECHONOLOGY\n",
            "Predicted keywords: facebook_techcrunch_twitter_thehill_data\n",
            "*****************************\n",
            "Test text: Phoenix Suns favorites to win NBA title, but they still feel disrespected. Are they overlooked?\n",
            "Predicted Topic: WORLD AFFAIRS\n",
            "Predicted keywords: preview_win_past_nfl_olympics\n",
            "*****************************\n",
            "Test text: Natural gas spikes to highest level since 2008 as rare nor'easter looms\n",
            "Predicted Topic: ENERGY\n",
            "Predicted keywords: oil_saudi_huawei_crude_update\n",
            "*****************************\n",
            "Test text: Will rising prices sink Biden’s midterm hopes for Democrats?\n",
            "Predicted Topic: POLTICS\n",
            "Predicted keywords: thehill_trump_trumps_gop_clinton\n",
            "*****************************\n",
            "Test text: Large and dangerous' tornadoes hit Texas and Oklahoma; South faces more severe weather\n",
            "Predicted Topic: CLIMATE\n",
            "Predicted keywords: hurricane_california_marijuana_thehill_storm\n",
            "*****************************\n",
            "Test text: Here is a list of the best beaches in Hawaii and other tropical islands\n",
            "Predicted Topic: LIFESTYLE\n",
            "Predicted keywords: dog_new_space_food_drone\n",
            "*****************************\n"
          ]
        }
      ]
    }
  ]
}